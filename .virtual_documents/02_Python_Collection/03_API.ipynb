


## JSON 활용 연습



import requests, json, pandas as pd


URL = 'http://www.krei.re.kr:18181/chart/main_chart/index/kind/W/sdate/2019-01-01/edate/2019-12-31'
resp = requests.get(URL)
resp


data = resp.text
print(type(data), len(data))
print(data)


data2 = resp.json()
print(type(data2), len(data2))
print(data2)


data2








import requests
from bs4 import BeautifulSoup


# http://openapi.molit.go.kr:8081/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTrade?LAWD_CD=11110&DEAL_YMD=201512&serviceKey=%2BmCLvVL%2BZFZ%2FJpLQ9eWAHLDXiJIt9z6nUpdrRZSJKC5wbs9V%2FbD3xDGT8FZYxzaQxWb07W8VhNd4Hy3SvTnLnw%3D%3D
date = '201512'
region_code = '11110'
key = "%2BmCLvVL%2BZFZ%2FJpLQ9eWAHLDXiJIt9z6nUpdrRZSJKC5wbs9V%2FbD3xDGT8FZYxzaQxWb07W8VhNd4Hy3SvTnLnw%3D%3D"

url = f'http://openapi.molit.go.kr:8081/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTrade?LAWD_CD={region_code}&DEAL_YMD={date}&serviceKey={key}'

resp = requests.get(url)
textsrc = resp.text
textsrc


soup = BeautifulSoup(textsrc, 'xml')
soup


items = soup.select('response > body > items > item')
items


items[0]


items[0]


a = items[0]. select_one('거래금액')
print(type(a), a)


a.text





for item in items:
    print(item.select_one('거래금액').text.strip(),
          item.select_one('건축년도').text,
          item.select_one('년').text,
          item.select_one('법정동').text.strip(),
          item.select_one('아파트').text,
          item.select_one('월').text,
          item.select_one('일').text,
          item.select_one('전용면적').text,
          item.select_one('지번').text,
          item.select_one('지역코드').text,
          item.select_one('층').text)





def default_text(node, text):
    if node != None:
        return node.text.strip()
    else:
        return text


item_list = ['거래금액', '건축년도', '년', '법정동', '아파트', '월', '일', '전용면적', '지번', '지역코드', '층']
for item in items:
    for tag in item_list:
        print(default_text(item.select_one(tag), ''), end=' ')
    print()


def default_text(node, text):
    if node != None:
        return node.text.strip()
    else:
        return text

date = '201512'
region_code = '11110'
apikey = 'QjITnZtxSg5%2Bhzh%2BWR8hYLMstCDRuf1REcb5E59648Wy77%2B7z8aQBHgv95ylOhyoP31mFZWlyiqd2TrMu7HTuw%3D%3D'
url = f'http://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTradeDev?LAWD_CD={region_code}&DEAL_YMD={date}&serviceKey={apikey}'

res = requests.get(url)
textsrc = res.text

soup = BeautifulSoup(textsrc, 'xml')
items = soup.select('response > body > items > item') 

item_list = ['거래금액', '건축년도', '년', '법정동', '아파트', '월', '일', '전용면적', '지번', '지역코드', '층']
data_list = []

for item in items:
    data_list.append([ default_text(item.select_one(tag), '') for tag in item_list]) # value를 행기준으로 맞춤, 110 X 11
print(data_list)

df = pd.DataFrame(data_list, columns=item_list)
print('='*50)
print(df)
df.to_csv('./sample/아파트매매실거래자료수집.csv', index=False, encoding='cp949') 

















import requests, time, os, json
from html import unescape


# input
client_id = 've7cfMbnwwUb1RzCEHj4'
client_secret = 'i8GhJ9BiFa'

queries = ['전주 여행', '경주 여행']
goal_page = 5


# setting
user_agent = "Mozilla/5.0 (Windows NT 10.0; WOW64) " + \
             "AppleWebKit/537.36 (KHTML, like Gecko) " + \
             "Chrome/51.0.2704.103 Safari/537.36"

headers = {"User-Agent": user_agent,
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret}


    file_name = 'output/naver_kin.txt'

with open(file_name, 'w', encoding='utf-8') as f :
    f.write('query\tno\ttitle\tlink\tdescription\ttotal_text\n')


url = "https://openapi.naver.com/v1/search/kin.json?display=100&query=" + queries[0] + "&start=" + str(1)
response = requests.get(url, headers=headers)
response


print(response.text)


json.loads(response.text)


json.loads(response.text)['items']


elements = json.loads(response.text)['items']
elements[0]


def get_list(query, page):
    print('='*5, query, page, '='*5)
    url = "https://openapi.naver.com/v1/search/kin.json?display=100&query=" + query + "&start=" + str(page+1)
    response = requests.get(url, headers=headers)
    elements = json.loads(response.text)['items']

    for i, elm in enumerate(elements):
        title = elm['title'].replace("<b>", "").replace("</b>", "")
        title = unescape(title) # escape된 문자를 unescape문자로 변경
        link = elm['link']
        description = unescape(elm['description'].replace("<b>", "").replace("</b>", ""))
        description = unescape(description)
        
        print([query, (page*100)+(i+1), title, link, description, title+" "+description])

        with open(file_name, 'a', encoding='utf-8') as f: # overwrite 안되도록 add할 것
            f.write( f'{query}\t{(page*100)+(i+1)}\t{title}\t{link}\t{description}\t{title+" "+description}\n')

    return


for query in queries:
    for page in range(goal_page):
        kin_list = get_list(query, page)
        time.sleep(0.5) #웹페이지 크롤링 매너 최소 6초








# 셀레니움 라이브러리 설치
!pip install selenium


from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time

option = Options()
option


option.add_experimental_option("detach", True) #특정함수안에서  드라이버 생성시 함수종료될 때 브라우저 같이 종료되는 문제 대응





url = 'https://naver.com'
driver = webdriver.Chrome(options=options) # 객체 생성


driver.get(url) # 실행 
time.sleep(2) 





driver.back()


driver.forward()



driver.refresh()






title = driver.title
url = driver.current_url
handle = driver.current_window_handle
print(title, url, handle)





driver.find_element?


driver.find_element(By.ID, 'query')


# 키 입력
driver.find_element(By.ID, 'query').send_keys('뉴진스')


driver.find_element(By.CSS_SELECTOR, '#query').send_keys('에스파')


driver.find_element(By.XPATH, '//*[@id="query"]').send_keys('BTS')





driver.find_element(By.LINK_TEXT, '쇼핑').click()


# 일부 매칭
driver.find_element(By.PARTIAL_LINK_TEXT , '증').click()


# 태그는 너무 요소가 많으므로, 정확하게 대상을 찾을 때에는 안씀
driver.find_element(By.TAG_NAME, 'div')


# 여러개의 요소를찾을 때
links = driver.find_elements(By.CSS_SELECTOR, '.link_service')


for link in links:
    print(link)


driver.find_elements(By.CSS_SELECTOR, '.link_service')[0].get_attribute('href')


driver.find_elements(By.CSS_SELECTOR, '.link_service')[3].get_attribute('href')


for link in links:
    print(link.get_attribute('href'))





# 테스트용 html
# url = 'file:///C:/workspace/WASSUP3/02_Data_Collection/sample/signin.html'
url = 'file:///C:/workspace/wassup3/02_Python_Collection/sample/signin.html'
driver = webdriver.Chrome(options=options)
driver.get(url)
time.sleep(2) 


username = driver.find_element(By.NAME, "username")
username.send_keys('Korea')


password = driver.find_element(By.NAME, "password")
password.send_keys('1234')


login = driver.find_element(By.XPATH, '//*[@id="loginForm"]/input[3]')
login.click()


driver.back()


login = driver.find_element(By.XPATH, '/html/body/form/input[3]')
login.click()


driver.back()


username.clear()


password.clear()


driver.find_element(By.TAG_NAME, 'p').text


driver.page_source


driver.close()


from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time

options = Options()
options.add_argument("--headless=new") 
# options.add_argument('--window-size= x, y') #실행되는 브라우저 크기를 지정할 수 있습니다.
# options.add_argument('--start-maximized') #브라우저가 최대화된 상태로 실행됩니다.
# options.add_argument('--start-fullscreen') #브라우저가 풀스크린 모드(F11)로 실행됩니다.
# options.add_argument('--blink-settings=imagesEnabled=false') #브라우저에서 이미지 로딩을 하지 않습니다.
# options.add_argument('--mute-audio') #브라우저에 음소거 옵션을 적용합니다.
# options.add_argument('incognito') #시크릿 모드의 브라우저가 실행됩니다.
options.add_experimental_option("detach", True) #특정함수안에서  드라이버 생성시 함수종료될 때 브라우저 같이 종료되는 문제 대응
